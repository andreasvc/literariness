#!/usr/bin/python3
"""Evaluate predictions of a regression model based on various textual features
such as tree fragments and n-gram features.

Usage:

    python3 predict.py <dataset>

Where `<dataset>` is a directory containing feature counts and parameters.

Input
-----
Expects feature counts generated by features.py in the `features/` directory.
Reads a list of features to use from a file named `selectedfeatures.txt`
(a list of strings in Python syntax).

Additionally supports the following extra feature types, automatically used if
corresponding filenames are found:

- cliches: a tab-separated file 'cliches.tsv' with the number of cliche
    expressions per text.
- topics: a tab-separated file 'topics_avgoverchunks.tsv' of topic weights
    per text from an LDA model. 'Average' since the topic model in question
	was created from texts divided into chunks, with this file containing
	the mean weight for each text.

Output
------
Evaluation is printed to standard output.
Prediction results for bigrams, character n-grams, and tree fragments are
written to `pred.csv`.
"""
import warnings
warnings.simplefilter("ignore", UserWarning)
warnings.simplefilter("ignore", DeprecationWarning)
import os
import sys
import glob
import pandas
from sklearn import feature_extraction, preprocessing, metrics, pipeline, \
		svm, linear_model
from sklearn.externals import joblib
from features import load_sparse_mat, isdiscrete

norm = 'l2'
USE_IDF = False
SMOOTH_IDF = False
SUBLINEAR_TF = False

def evalreport(y_true, y_pred, target):
	"""Given two Series objects aligned with 'target', compute metrics for each
	fold and report mean/stderr."""
	result = pandas.DataFrame(index=['mean', 'std err'])
	ids = [target[target.fold == n].index for n in target.fold.unique()]
	if isdiscrete(target.target):
		acc = pandas.Series([metrics.accuracy_score(y_true[a], y_pred[a])
			for a in ids])
		result['Accuracy'] = 100 * pandas.Series([
				acc.mean(), acc.sem()], index=result.index)
	else:
		# NB: calculate mean of MSE of each fold, then take root of overall
		# mean. http://stats.stackexchange.com/a/85517
		mse = pandas.Series([metrics.mean_squared_error(y_true[a], y_pred[a])
				for a in ids])
		r2 = pandas.Series([metrics.r2_score(y_true[a], y_pred[a])
			for a in ids])
		tau = pandas.Series([y_true[a].corr(y_pred[a], method='kendall')
				for a in ids])
		result['$R^2$'] = 100 * pandas.Series([
				r2.mean(), r2.sem()], index=result.index)
		result[r'Kendall $\tau$'] = pandas.Series([
				tau.mean(), tau.sem()], index=result.index)
		result['RMS error'] = (pandas.Series([
				mse.mean(), mse.sem()], index=result.index) ** 0.5)
	return result.T


def evalreport1(y_true, y_pred):
	"""Simpler version to be used on one fold at a time."""
	if isdiscrete(y_true):
		return pandas.Series(
				data=[100 * metrics.accuracy_score(y_true, y_pred)],
				index=['Accuracy'])
	return pandas.Series(
			data=[100 * metrics.r2_score(y_true, y_pred),
				y_true.corr(y_pred, method='kendall'),
				metrics.mean_squared_error(y_true, y_pred)],
			index=['$R^2$', r'Kendall $\tau$', 'MSE'])


def inducedfeatures():
	"""Run experiments using bag-of-{words,char-n-grams,fragments} models
	and store predictions in pred.csv."""
	# fixed regularization parameters.
	C = 100
	epsilon = 0

	# use simple.index to filter out texts with less than 1000 sentences
	# for which we don't have counts.
	simple = pandas.read_csv(
			'features/simple.csv.gz', index_col=0, header=[0, 1])
	target = pandas.read_csv('features/target.csv', index_col=0)
	target = target.ix[target.index & simple.index]

	result = pandas.DataFrame(index=target.index)
	result['fold'] = target.fold
	result['target'] = target.target

	discrete = isdiscrete(target.target)
	le = preprocessing.LabelEncoder()
	if discrete:
		le.fit(target.target)

	# n-gram features
	for featclass in glob.glob('features/*.csv.gz'):
		if featclass == 'features/simple.csv.gz':
			continue
		print('\n%s' % featclass)
		data = pandas.read_csv(featclass, index_col=0).T.ix[target.index]
		data.fillna(0, inplace=True)
		if len(data.columns) > 1000:
			mindocs, maxdocs = int(0.1 * len(data)), int(0.9 * len(data))
			docsperfeature = (data > 0).sum(axis=0)
			features = data.loc[:, (docsperfeature > mindocs)
					& (docsperfeature < maxdocs)]
		else:
			features = data
		print(data.shape, features.shape)
		model = pipeline.Pipeline([
				('tfidf', feature_extraction.text.TfidfTransformer(
					norm='l2', use_idf=USE_IDF, smooth_idf=SMOOTH_IDF,
					sublinear_tf=SUBLINEAR_TF)),
				('svm', svm.LinearSVC(C=C, random_state=1)
					if discrete else
					svm.LinearSVR(C=C, epsilon=epsilon,
						loss='epsilon_insensitive', random_state=1))])
		X = features.values
		y_pred = pandas.Series(index=target.index)
		y_true = target.target
		for n in sorted(target.fold.unique()):
			infold = target.fold == n
			outsidefold = target.fold != n
			X_train = X[outsidefold.nonzero()[0], :]
			X_test = X[infold.nonzero()[0], :]
			y_train = target[outsidefold].target
			if discrete:
				y_train = le.transform(y_train)
			model.fit(X_train, y_train)
			if n == 1:
				joblib.dump((features.columns, model),
						featclass.split('.')[0] + '.pkl')
			foldpred = model.predict(X_test)
			if discrete:
				foldpred = le.inverse_transform(foldpred)
			y_pred[infold] = foldpred
		basename = os.path.basename(featclass).replace('.csv.gz', '')
		result[basename] = pandas.Series(y_pred, index=target.index)
		print(evalreport(y_true, result[basename], target).round(decimals=3))

	# fragments
	if os.path.exists('features/fragcounts.npz'):
		print('\nfragments')
		fragcounts, index, columns = load_sparse_mat('features/fragcounts.npz')
		print(fragcounts.shape)
		index = pandas.Index([os.path.splitext(a)[0] for a in index])
		fragcounts = fragcounts[index.get_indexer(target.index), :]
		print(fragcounts.shape)
		selections = {n: columns.isin(
					{line[:line.index('\t')] for line in
						open('features/rankednonredundantfragfold%d.txt' % n)})
				for n in target.fold.unique()}
		y_pred = pandas.Series(index=target.index)
		y_true = target.target
		scaler = feature_extraction.text.TfidfTransformer(
				norm='l2', use_idf=USE_IDF, smooth_idf=SMOOTH_IDF,
				sublinear_tf=SUBLINEAR_TF)
		predmodel = (svm.LinearSVC(C=C, random_state=1)
				if discrete else
				svm.LinearSVR(C=C, epsilon=epsilon,
					loss='epsilon_insensitive', random_state=1))
		model = pipeline.Pipeline([('scaler', scaler), ('svm', predmodel)])
		for n in sorted(target.fold.unique()):
			infold = target.fold == n
			outsidefold = target.fold != n
			fragoutsidefold = selections[n]
			X = fragcounts[:, fragoutsidefold]
			X_train = X[outsidefold.nonzero()[0], :]
			X_test = X[infold.nonzero()[0], :]
			print(n, X_train.shape, X_test.shape)
			y_train = target[outsidefold].target
			if discrete:
				y_train = le.transform(y_train)
			model.fit(X_train, y_train)
			if n == 1:
				joblib.dump(model, 'fragments.pkl')
			foldpred = model.predict(X_test)
			if discrete:
				foldpred = le.inverse_transform(foldpred)
			y_pred[infold] = foldpred
		result['fragments'] = y_pred
		print(evalreport(y_true, y_pred, target).round(decimals=3))

	# scores per fold
	print('\n', pandas.DataFrame({x: [evalreport1(
			result.loc[result.fold == n, 'target'],
			result.loc[result.fold == n, x])[
				'Accuracy' if discrete else '$R^2$']
			for n in sorted(result.fold.unique())]
			for x in result.columns if x not in ('fold', 'target')},
			index=sorted(result.fold.unique())).round(decimals=3).T, '\n')
	result.to_csv('pred.csv')


def ensemble():
	"""Combine predictions of automatically induced features
	with other features."""
	pred = pandas.read_csv('pred.csv', index_col=0)
	pred.index = [a.replace('.dbr', '').replace('.mrg', '')
			for a in pred.index]
	selected = pred.index
	metadata = pandas.read_csv('metadata.csv', index_col='Label')
	simple = pandas.read_csv(
			'features/simple.csv.gz', index_col=0, header=[0, 1])
	simple.fillna(0, inplace=True)
	simpleorig = simple.copy()
	# flatten columns
	simple.columns = simple.columns.get_level_values(1)
	if os.path.exists('features/cliches.tsv'):
		cliches = pandas.read_table('features/cliches.tsv', index_col=0
				).rename(columns={'@ 10,000 sentences': 'cliches'})
	else:
		cliches = pandas.DataFrame(index=selected)
	if os.path.exists('features/topics_avgoverchunks.tsv'):
		topics = pandas.read_table('features/topics_avgoverchunks.tsv',
				index_col=0)
	else:
		topics = pandas.DataFrame(index=selected)
	if os.path.exists('features/baselinefeatures.csv'):
		base = pandas.read_csv('features/baselinefeatures.csv', index_col=0)
	else:
		base = pandas.DataFrame(index=selected)
	data = pandas.concat([metadata, simple, cliches, topics,
			base[base.columns.difference(metadata.columns)]],
			axis=1).ix[selected]
	target = pandas.read_csv('features/target.csv', index_col=0)
	target = target.ix[selected]
	discrete = isdiscrete(target.target)
	le = preprocessing.LabelEncoder()
	if discrete:
		le.fit(target.target)
	with open('selectedfeatures.txt', 'r') as inp:
		feats = eval(inp.read())
	y = target.target
	print('N =', len(y))
	# dict vectorizer is used to transform categorical variables
	# into indicator variables
	dv = feature_extraction.DictVectorizer(sparse=False, sort=False)
	if discrete:
		scaler = feature_extraction.text.TfidfTransformer(
				norm='l2', use_idf=USE_IDF, smooth_idf=SMOOTH_IDF,
				sublinear_tf=SUBLINEAR_TF)
		predmodel = linear_model.LogisticRegressionCV(random_state=1)
		model = pipeline.Pipeline([
				('dv', dv), ('scaler', scaler), ('predmodel', predmodel)])
	else:
		# NB: don't apply scaler because RidgeCV has normalize=True
		predmodel = linear_model.RidgeCV(normalize=True)
		model = pipeline.Pipeline([('dv', dv), ('predmodel', predmodel)])
	def runseq(seq):
		"""Train a model on the features in `seq` and return vector with
		predictions."""
		y_pred = pandas.Series(index=target.index)
		for fold in sorted(target.fold.unique()):
			selected = pandas.DataFrame()
			for a in seq:
				x = feats[a]
				if x == 'topics':  # add columns for all topics
					tmp = data[data.columns[data.columns.str.match(
							r't[0-9]{1,2}', as_indexer=True)]]
					assert tmp.shape[1] > 0
					selected[tmp.columns] = tmp
				elif x in simpleorig.columns.levels[0]:  # add feature class
					tmp = simpleorig[x]
					selected[tmp.columns] = tmp
				elif x in pred.columns:  # add prediction from submodel
					# ('bigrams', 'fragments', 'char4grams')
					selected[x] = pred[x]
				elif x in data.columns:  # add a single column
					selected[x] = data[x]
				else:
					raise ValueError('unknown feature %r' % x)
			infold = target.fold == fold
			outsidefold = target.fold != fold
			X_train = [row for _, row in selected.ix[outsidefold].iterrows()]
			X_test = [row for _, row in selected.ix[infold].iterrows()]
			y_train = target[outsidefold].target
			if discrete:
				y_train = le.transform(y_train)
			model.fit(X_train, y_train)
			if fold == 1 and len(seq) == len(feats):
				joblib.dump(model, 'ensemble.pkl')
			foldpred = model.predict(X_test)
			if discrete:
				foldpred = le.inverse_transform(foldpred)
			y_pred[infold] = foldpred
		return y_pred

	res = pandas.DataFrame(index=['Accuracy'] if discrete
			else ['$R^2$', r'Kendall $\tau$', 'RMS error'])
	result = pandas.DataFrame(index=target.index)
	result['fold'] = target.fold
	result['target'] = target.target
	for n, _ in enumerate(feats):  # add one feature at a time, i.e., 0:n+1
		y_pred = runseq(range(0, n + 1))
		df = evalreport(y, y_pred, target)
		res[('+ ' if n else '') + feats[n]] = df['mean']
		result[('+ ' if n else '') + feats[n]] = y_pred

	print(res.T.round(decimals=3))  # .to_latex())
	result.to_csv('ensemble.csv')


if  __name__ == '__main__':
	try:
		_, datasetdir = sys.argv
		os.chdir(datasetdir)
	except (ValueError, FileNotFoundError):
		print(__doc__)
		sys.exit(2)

	inducedfeatures()
	ensemble()
